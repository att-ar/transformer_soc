{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Iw1qCBlT-z"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqXowf9MlT-1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh1JdQmwlT-3"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/transformer_soc/rolling_and_plot_tf.py .\n",
    "!cp /content/drive/MyDrive/transformer_soc/sim_data.csv .\n",
    "!cp /content/drive/MyDrive/transformer_soc/transformer_helper.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "# from os import environ\n",
    "# environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "# removes tensorflow warnings triggered because of Tensorflow incompatibility with my Apple M1 chip.\n",
    "# ignore this when using a non Apple Silicon device, ie. Google Colab or the likes.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Input, Dropout, BatchNormalization\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DOA-JbhlT-4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install jupyterplot\n",
    "from jupyterplot import ProgressPlot as PP\n",
    "\n",
    "from transformer_helper import *\n",
    "from rolling_and_plot_tf import data_plot, rolling_split, normalize, validate\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvorie1ElT-5"
   },
   "source": [
    "Will have to figure out how to set device to cuda in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUteRx9dlT-5"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Import](#0)\n",
    "- [Preprocessing](#win)\n",
    "- [Model](#model)\n",
    " - [Encoder](#enc)\n",
    "    - [Encoder Layer](#enc-lay)\n",
    "    - [Full Encoder](#full-enc)\n",
    " - [Decoder](#dec)\n",
    "    - [Decoder Layer](#dec-lay)\n",
    "    - [Full Decoder](#full-dec)\n",
    " - [Transformer](#transform)\n",
    "- [Loss and Learn Rate Scheduler](#loss)\n",
    "- [Training](#train)\n",
    "- [Validate](#val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EL21GdslT-5"
   },
   "source": [
    "# Literature:\n",
    "\n",
    "\n",
    "According to [A Transformer-based Framework for Multivariate Time Series Representation Learning](https://dl.acm.org/doi/abs/10.1145/3447548.3467401):\n",
    "Using **Batch Normalization is significantly more effective** for multivariate time-series than using the traditional Layer Normalization method found in NLP.\n",
    "\n",
    "In addition, according to [Deep learning approach towards accurate state of charge estimation for lithium-ion batteries using self-supervised transformer model](https://www.nature.com/articles/s41598-021-98915-8#Sec9):\n",
    "Using a transformer network while **forgoing the Decoder Layer** is more effective for the application of State-of-Charge estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "$\\large{Self\\ Attention}$\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2DSwSOZlT-7"
   },
   "source": [
    "$\\large{Input}$\n",
    "\n",
    "Voltage, Current, SOC at times:\n",
    "$$t - window\\_size - 1 \\rightarrow t - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw-WpE1ulT-9"
   },
   "source": [
    "**Note**\n",
    "\n",
    "Cannot use embedding layers with battery data because of floating point values and negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WStD-7ytlT-9"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class G:\n",
    "    #preprocess\n",
    "    window_time = 96 #seconds\n",
    "    window_size = 32\n",
    "    slicing = window_time // window_size\n",
    "    batch_size = 16\n",
    "    #network\n",
    "    dense_dim = 32\n",
    "    model_dim = 128\n",
    "    num_features = 3 # current, voltage, and soc at t minus G.window_size -> t minus 1\n",
    "    num_heads = 16\n",
    "    num_layers = 6\n",
    "    #training\n",
    "    epochs = 100\n",
    "    learning_rate = 0.0035\n",
    "    min_learning_rate = 7e-11\n",
    "#     weight_decay = 0.0 #No weight decay param in the the keras optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prIueTe-lT-9"
   },
   "source": [
    "<a id=\"win\"></a>\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "il6DI4Z7lT--"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "file = pd.read_csv(\"/content/sim_data.csv\")\n",
    "#if using sim_data.csv:\n",
    "file[\"soc\"] *= 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLQrFOvrlT--"
   },
   "outputs": [],
   "source": [
    "data_plot(data = [file],\n",
    "          title=\"OCV v SOC\",\n",
    "          x = [\"test time (sec)\"],\n",
    "          y = [\"soc\"],\n",
    "          markers = \"lines\",\n",
    "          color = \"darkorchid\",\n",
    "          x_title = \"Test Time (sec)\",\n",
    "          y_title = \"SOC\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f7QighFlT--"
   },
   "outputs": [],
   "source": [
    "file = normalize(file.loc[:,[\"current\",\"voltage\",\"soc\"]].iloc[::G.slicing])\n",
    "#uses sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x79KvZ3ilT--"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = rolling_split(file, G.window_size, train=True)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "#uses sklearn.model_selection\n",
    "\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "train_dataloader = tf.data.Dataset.zip((x_train, y_train)).batch(G.batch_size, drop_remainder=True)\n",
    "test_dataloader = tf.data.Dataset.zip((x_test, y_test)).batch(G.batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRmivoyVlT-_"
   },
   "outputs": [],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(f\"Shape of X [window, features]: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"model\"></a>\n",
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "def FullyConnected():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(G.dense_dim, activation='relu',\n",
    "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.005, maxval = 0.08)\n",
    "                             ),\n",
    "        # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4),\n",
    "        tf.keras.layers.Dense(G.num_features, activation='relu',\n",
    "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                             ),\n",
    "        # (G.batch_size, G.window_size, G.num_features)\n",
    "        tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='enc'></a>\n",
    "## Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='enc-lay'></a>\n",
    "###  Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_heads,\n",
    "                 num_features,\n",
    "                 dense_dim,\n",
    "                 dropout_rate=0.3,\n",
    "                 batchnorm_eps=1e-3):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = num_features,\n",
    "            dropout = dropout_rate,\n",
    "            kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "            kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "            bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                                     )\n",
    "        #feed-forward-network\n",
    "        self.ffn = FullyConnected()\n",
    "        \n",
    "        \n",
    "        self.batchnorm1 = BatchNormalization(momentum = 0.9, epsilon=batchnorm_eps)\n",
    "        self.batchnorm2 = BatchNormalization(momentum = 0.85, epsilon=batchnorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "        Returns:\n",
    "            encoder_layer_out -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "        \"\"\"\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        attn_output = self.mha(query = x,\n",
    "                               value = x) # Self attention\n",
    "        \n",
    "        out1 = self.batchnorm1(tf.add(x, attn_output))  # (G.batch_size, G.window_size, G.num_features)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        \n",
    "        if training:\n",
    "            ffn_output = self.dropout_ffn(ffn_output) # (G.batch_size, G.window_size, G.num_features)\n",
    "        \n",
    "        encoder_layer_out = self.batchnorm2(tf.add(ffn_output, out1))\n",
    "        # (G.batch_size, G.window_size, G.num_features)\n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKgObFUUlT_B"
   },
   "source": [
    "<a name='full-enc'></a>\n",
    "### Full Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self,\n",
    "                 num_layers = G.num_layers,\n",
    "                 num_heads = G.num_heads,\n",
    "                 num_features = G.num_features,\n",
    "                 dense_dim = G.dense_dim,\n",
    "                 input_size = G.num_features,\n",
    "                 maximum_position_encoding = G.window_size,\n",
    "                 dropout_rate=0.35,\n",
    "                 batchnorm_eps=1e-6):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                input_size)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(num_heads = num_heads,\n",
    "                                        num_features = num_features,\n",
    "                                        dense_dim = dense_dim,\n",
    "                                        dropout_rate = dropout_rate,\n",
    "                                        batchnorm_eps = batchnorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            out2 -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "        \"\"\"\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        if training: x = self.dropout(x)\n",
    "        \n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x,training)\n",
    "            \n",
    "        # only need the final time's data : time = t-1 from the window\n",
    "        # x has shape (G.batch_size, G.window_size, G.num_features)\n",
    "        # but I am only returning time t-1:\n",
    "        return x[:, -1, :] # (G.batch_size, G.num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='dec'></a> \n",
    "## Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='dec-lay'></a> \n",
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The decoder layer is composed by two multi-head attention blocks, \n",
    "    one that takes the new input and uses self-attention, and the other \n",
    "    one that combines it with the output of the encoder, followed by a\n",
    "    fully connected block. \n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_heads,\n",
    "                 num_features,\n",
    "                 dense_dim,\n",
    "                 dropout_rate=0.3,\n",
    "                 batchnorm_eps=1e-3):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = num_features,\n",
    "            dropout = dropout_rate,\n",
    "            kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "            kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "            bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                                     )\n",
    "\n",
    "        self.mha2 = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = num_features,\n",
    "            dropout = dropout_rate,\n",
    "            kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "            kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "            bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                                     )\n",
    "\n",
    "        self.ffn = FullyConnected()\n",
    "\n",
    "        self.batchnorm1 = BatchNormalization(momentum = 0.95, epsilon=batchnorm_eps)\n",
    "        self.batchnorm2 = BatchNormalization(momentum = 0.9, epsilon=batchnorm_eps)\n",
    "        self.batchnorm3 = BatchNormalization(momentum = 0.85, epsilon=batchnorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, y, enc_output, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the Decoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            y -- Tensor of shape (G.batch_size, 1) #the soc values for the batches\n",
    "            enc_output --  Tensor of shape(G.batch_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout and batchnorm layers\n",
    "        Returns:\n",
    "            out3 -- Tensor of shape (G.batch_size, 1)\n",
    "            attn_weights_block1\n",
    "            attn_weights_block2\n",
    "        \"\"\"\n",
    "        \n",
    "        # BLOCK 1\n",
    "        # Dropout will be applied during training only\n",
    "        mult_attn_out1, attn_weights_block1 = self.mha1(query = y[np.newaxis,:],\n",
    "                                                        value = y[np.newaxis,:],\n",
    "                                                        return_attention_scores=True)\n",
    "        # (G.batch_size, G.num_features)\n",
    "        \n",
    "        Q1 = self.batchnorm1(tf.add(y,mult_attn_out1))\n",
    "\n",
    "        # BLOCK 2\n",
    "        # calculate self-attention using the Q from the first block and K and V from the encoder output. \n",
    "        # Dropout will be applied during training\n",
    "        mult_attn_out2, attn_weights_block2 = self.mha2(query = Q1,\n",
    "                                                        value = enc_output[np.newaxis,:],\n",
    "                                                        key = enc_output[np.newaxis,:],\n",
    "                                                        return_attention_scores=True)\n",
    "        \n",
    "        mult_attn_out2 = self.batchnorm2( tf.add(mult_attn_out1, mult_attn_out2) )\n",
    "                \n",
    "        #BLOCK 3\n",
    "        # pass the output of the second block through a ffn\n",
    "        ffn_output = self.ffn(mult_attn_out2)\n",
    "        \n",
    "        # apply a dropout layer to the ffn output\n",
    "        if training:\n",
    "            ffn_output = self.dropout_ffn(ffn_output)\n",
    "            \n",
    "        out3 = self.batchnorm3( tf.add(ffn_output, mult_attn_out2) )\n",
    "        return tf.squeeze(out3,axis=0), attn_weights_block1, attn_weights_block2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='full-dec'></a> \n",
    "### Full Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder is starts by passing the target input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    decoder Layers\n",
    "        \n",
    "    \"\"\" \n",
    "    def __init__(self,\n",
    "                 num_layers = G.num_layers,\n",
    "                 num_heads = G.num_heads,\n",
    "                 num_features = G.num_features,\n",
    "                 dense_dim = G.dense_dim,\n",
    "                 target_size = G.num_features,\n",
    "                 maximum_position_encoding = 1,\n",
    "                 dropout_rate=0.35,\n",
    "                 batchnorm_eps=1e-6):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "#         self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "#                                                 target_size)\n",
    "        # I don't need positional encoding for a single value label\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(num_heads,\n",
    "                                        num_features,\n",
    "                                        dense_dim) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, y, enc_output, training):\n",
    "        \"\"\"\n",
    "        Forward  pass for the Decoder\n",
    "        \n",
    "        Arguments:\n",
    "            y -- Tensor of shape (G.batch_size, 1) #the SOC values for the batches\n",
    "            enc_output --  Tensor of shape(G.batch_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "        Returns:\n",
    "            y -- Tensor of shape (G.batch_size, 1)\n",
    "            attention_weights - Dictionary of tensors containing all the attention weights\n",
    "        \"\"\"\n",
    "\n",
    "        seq_len = tf.shape(y)[1]\n",
    "        attention_weights = {}\n",
    "        \n",
    "#         y += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        # apply a dropout layer to y\n",
    "        if training: y = self.dropout(y)\n",
    "\n",
    "        # use a for loop to pass y through a stack of decoder layers and update attention_weights\n",
    "        for i in range(self.num_layers):\n",
    "            # pass y and the encoder output through a stack of decoder layers and save attention weights\n",
    "            y, block1, block2 = self.dec_layers[i](y, enc_output, training)\n",
    "\n",
    "            #update attention_weights dict\n",
    "            attention_weights[f'decoder_layer{i+1}_block1_self_att'] = block1\n",
    "            attention_weights[f'decoder_layer{i+1}_block2_decenc_att'] = block2\n",
    "            \n",
    "        return y, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U2F58rnlT_C"
   },
   "source": [
    "<a name='transform'></a> \n",
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHymPmaj-2ba"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_layers = G.num_layers,\n",
    "                 num_heads = G.num_heads,\n",
    "                 dense_dim = G.dense_dim,\n",
    "                 max_positional_encoding_input = G.window_size,\n",
    "                 max_positional_encoding_target = G.window_size):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "\n",
    "        self.final_stack = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(\n",
    "                dense_dim, activation = \"relu\",\n",
    "                kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.02)\n",
    "                                  ),\n",
    "            tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4),\n",
    "            tf.keras.layers.Dense(\n",
    "                1, activation = \"sigmoid\",\n",
    "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.005)\n",
    "                                 )\n",
    "                                              ])\n",
    "    \n",
    "    def call(self, x, y, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "                 An array of the windowed voltage, current and soc data\n",
    "            y -- Tensor of shape (G.batch_size, 1)\n",
    "                 An array of the SOC targets for each batch\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout and batchnorm layers\n",
    "        Returns:\n",
    "            final_output -- SOC prediction at time t\n",
    "        \n",
    "        \"\"\"\n",
    "        enc_output = self.encoder(x, training) # (G.batch_size, G.num_features)\n",
    "        \n",
    "        dec_output, attention_weights = self.decoder(y, enc_output, training) # (G.batch_size, G.num_features)\n",
    "\n",
    "        final_output = self.final_stack(dec_output) # (G.batch_size, 1)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = Transformer()\n",
    "x_inst = tf.random.uniform((G.batch_size, G.window_size, G.num_features))\n",
    "y_inst = tf.random.uniform((G.batch_size,1))\n",
    "model(x_inst,y_inst,False)\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading Already Saved Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUcLoUmWlT_D"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/drive/MyDrive/transformer_soc/decoder_model_weights.tf\")\n",
    "\n",
    "# attn_weights_load = pd.read_csv(\"/content/drive/MyDrive/transformer_soc/attn_weights.csv\").to_dict(\"list\")\n",
    "\n",
    "# scheduler_state = np.load(\"/content/drive/MyDrive/transformer_soc/scheduler_state.npy\")\n",
    "# print(f\"Saved learning_rate, T_cur, and T_i: {scheduler_state}\")\n",
    "\n",
    "# scheduler.learning_rate, scheduler.T_cur, scheduler.T_i = scheduler_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYtQv1TtlT_D"
   },
   "source": [
    "<a id = \"loss\"></a>\n",
    "# Loss and LR Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTN3TiSblT_D"
   },
   "source": [
    "**Learning Rate Scheduler**\n",
    "\n",
    "Cosine Annealing with Warm Restarts proposed by Loshchilov et al. in [SGDR: Stochastic Gradient Descent with Warm Restarts](https://doi.org/10.48550/arXiv.1608.03983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWt1eUd9o6WA"
   },
   "source": [
    "$$\\mu_t = \\mu_{min} + \\frac{1}{2}(\\mu_{max} - \\mu_{min})\\cdot (1 + \\cos (\\frac{T_{cur}}{T_i}\\pi))$$\n",
    "\n",
    "Where:\n",
    " - $\\mu$ is the learning_rate, subscript $t$ is for time = $t$\n",
    " - $T_{cur}$ is the number of epochs since the last restart\n",
    " - $T_i$ is the number of epochs between two restarts\n",
    "\n",
    "Note:\n",
    " - When $T_{cur} = T_i \\rightarrow \\mu_t = \\mu_{min}$\n",
    " - When $T_{cur} = 0 \\rightarrow \\mu_t = \\mu_{max}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ed637jTilT_D"
   },
   "outputs": [],
   "source": [
    "class CosAnnealWarmRestarts():\n",
    "    def __init__(self, T_0: float, T_mult: float):\n",
    "        '''\n",
    "        Cosine Annealing with Warm Restarts\n",
    "        Returns a new learning rate based on the call method\n",
    "        \n",
    "        Parameters:\n",
    "        `T_0` int\n",
    "            the number of iterations for the first restart to occur\n",
    "        `T_mult` int\n",
    "            the factor to increase T_i by after a restart, where T_i is the i^th restart.\n",
    "        '''\n",
    "        super(CosAnnealWarmRestarts, self).__init__()\n",
    "        \n",
    "        assert isinstance(T_0, float) and isinstance(T_mult, float)\n",
    "        assert T_0 > 0.0\n",
    "        self.mu_max = G.learning_rate #initial and max learning_rate\n",
    "        self.mu_min = G.min_learning_rate #minimum learning_rate\n",
    "        self.T_i = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.T_cur = 0.0\n",
    "        \n",
    "        self.learning_rate = G.learning_rate\n",
    "        \n",
    "    def step(self, increment:float, optimizer):\n",
    "        '''\n",
    "        Cosine Annealing with Warm Restarts\n",
    "        Returns a new learning rate based on the schedule described below\n",
    "        \n",
    "        Call after every batch\n",
    "\n",
    "        Parameters:\n",
    "        `increment` float\n",
    "            1 batch / total number of batches\n",
    "            !!!!! Not the current batch number, that would be a series summation\n",
    "            every epoch, a total of 1.0 will be added to self.T_cur\n",
    "        `optimizer`\n",
    "            the optimizer for the neural network\n",
    "        '''\n",
    "        try:\n",
    "            optimizer.learning_rate\n",
    "        except AttributeError:\n",
    "            print(\"Error: optimizer does not have a learning_rate parameter\")\n",
    "        \n",
    "        mu_i = self.mu_min + 0.5 * (\n",
    "                self.mu_max - self.mu_min) * (\n",
    "                    1 + tf.math.cos(np.pi * self.T_cur / self.T_i))\n",
    "        \n",
    "        self.T_cur += increment\n",
    "        \n",
    "        if np.isclose(self.T_cur, self.T_i):\n",
    "            self.T_i *= self.T_mult\n",
    "            self.T_cur = 0.0\n",
    "        \n",
    "        #update the learning_rate accordingly:\n",
    "        optimizer.learning_rate.assign(tf.cast(mu_i,tf.float32))\n",
    "        \n",
    "        self.learning_rate = mu_i\n",
    "        #this is just so that you can find the current learning rate from the scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg7FmZOHlT_E"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.LogCosh()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = G.learning_rate,\n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.999\n",
    "                                    )\n",
    "scheduler = CosAnnealWarmRestarts(T_0 = 1.0, T_mult = 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Train and Test Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-3MIaQslT_D"
   },
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y, training):\n",
    "    y_hat, attn_weights = model(x, y, training=training)\n",
    "    return y_hat, attn_weights, loss_object(y_true = y, y_pred = y_hat)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_hat, attn_weights, loss = loss_fn(model, inputs, targets, training=True)\n",
    "    return y_hat, attn_weights, loss, tape.gradient(loss, model.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ut8epLdhlT_E"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, training = True):\n",
    "    size = len(dataloader)\n",
    "    perc_error = 0.0\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for batch, (x,y) in enumerate(dataloader):\n",
    "        \n",
    "        predict, attn_weights, loss, grads = grad(model, x, y) # assert(loss.shape == [])\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "        scheduler.step(1 / size, optimizer)\n",
    "        \n",
    "        perc_error += tf.math.reduce_mean(tf.abs(predict - y) / (y + 1e-2) * 100, [0,1])\n",
    "        epoch_loss_avg.update_state(loss)\n",
    "        if batch % (size // 15) == 0:\n",
    "            print(f\"Mean loss: {epoch_loss_avg.result():>7f}  [{batch:4d}/{size:4d}]\")\n",
    "        elif batch == 3: break\n",
    "\n",
    "    perc_error /= size\n",
    "    print(f\"Train Error: \\nAverage Accuracy: {100 - perc_error}%\")\n",
    "    return epoch_loss_avg.result(), (100. - perc_error), attn_weights\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn, training = False):\n",
    "    size = len(dataloader)\n",
    "    perc_error = 0.0\n",
    "    counter = 0\n",
    "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
    "    \n",
    "    for x,y in dataloader:\n",
    "        predict, test_loss = loss_fn(model, x, y, training)\n",
    "        \n",
    "        if np.isnan(test_loss).any():\n",
    "            print(\"Test Loss had a np.nan value\")\n",
    "            break\n",
    "        \n",
    "        epoch_loss_avg.update_state(test_loss)\n",
    "        perc_error += tf.math.reduce_mean(tf.abs(predict - y) / (y + 1e-2) * 100, [0,1])\n",
    "\n",
    "        counter += 1\n",
    "        if counter % (size // 2) == 0:\n",
    "            print(f\"{counter} / {size} tested\")\n",
    "        elif counter == 3:\n",
    "            break\n",
    "            \n",
    "    perc_error /= size\n",
    "    print(f\"Test Error: \\nAverage Accuracy: {100 - perc_error}%, Avg Loss: {epoch_loss_avg.result():>8f}\\n\")\n",
    "    return epoch_loss_avg.result(), 100. - perc_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45--3qknlT_H"
   },
   "source": [
    "<a id = \"train\"></a>\n",
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3NZQjtqClT_H"
   },
   "outputs": [],
   "source": [
    "pp = PP(plot_names = [\"Mean Log Loss\", \"% Accuracy\"],\n",
    "        line_names = [\"Train Loop\", \"Test Loop\"],\n",
    "        x_label = \"epochs\"\n",
    "       )\n",
    "## Note the y-axis gets cut off with numbers longer than three digits because the source code has a bug\n",
    "## I checked the github repo for lr-curve and the issue has been raised but not closed\n",
    "\n",
    "for epoch in range(1, G.epochs+1):\n",
    "    print(f\"Epoch {epoch}/{G.epochs}\\n--------------------------------------\")\n",
    "    train_loss, train_acc, attn_weights = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler)\n",
    "    test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
    "    pp.update([[train_loss.numpy(), test_loss.numpy()], [train_acc, test_acc]])\n",
    "    \n",
    "#     if epoch % 15:\n",
    "#         model.save_weights(\"/content/drive/MyDrive/transformer_soc/decoder_model_weights.tf\", overwrite = True)\n",
    "#         pd.DataFrame(attn_weights).to_csv(\"/content/drive/MyDrive/transformer_soc/decoder_attn_weights.csv\",index=False)\n",
    "    \n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Progress**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yF6RygxlT_I"
   },
   "outputs": [],
   "source": [
    "model.save_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.tf\", overwrite = True)\n",
    "\n",
    "pd.DataFrame(attn_weights).to_csv(\"/content/drive/MyDrive/transformer_soc/attn_weights.csv\",index=False)\n",
    "\n",
    "np.save(\n",
    "    \"/content/drive/MyDrive/transformer_soc/scheduler_state.npy\",\n",
    "    np.array([scheduler.learning_rate.numpy(), scheduler.T_cur, scheduler.T_i])\n",
    "       )\n",
    "\n",
    "print(f'''\n",
    "lr: {scheduler.learning_rate.numpy()}\n",
    "T_cur: {scheduler.T_cur}\n",
    "T_i: {scheduler.T_i}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5pSwH7QlT_I"
   },
   "source": [
    "<a id = \"val\"></a>\n",
    "# Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYr2y9eulT_I"
   },
   "source": [
    "**Dev Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuY9saCblT_I"
   },
   "outputs": [],
   "source": [
    "visualize_dev = validate(model, test_dataloader, dev = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5uLkWkLlT_I"
   },
   "source": [
    "**Entire Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjvsvbIllT_I"
   },
   "outputs": [],
   "source": [
    "x_set, y_set = rolling_split(file, G.window_size, train = False)\n",
    "\n",
    "x_set = tf.data.Dataset.from_tensor_slices(x_set)\n",
    "y_set = tf.data.Dataset.from_tensor_slices(y_set)\n",
    "\n",
    "set_dataloader = tf.data.Dataset.zip((x_set, y_set)).batch(G.batch_size, drop_remainder=True)\n",
    "\n",
    "visualize = validate(model, set_dataloader, dev = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "transform_notebook.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
