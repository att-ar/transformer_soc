{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_Iw1qCBlT-z"
   },
   "source": [
    "<a name='0'></a>\n",
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqXowf9MlT-1"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eh1JdQmwlT-3"
   },
   "outputs": [],
   "source": [
    "!cp /content/drive/MyDrive/transformer_soc/rolling_and_plot_tf.py .\n",
    "!cp /content/drive/MyDrive/transformer_soc/sim_data.csv .\n",
    "!cp /content/drive/MyDrive/transformer_soc/transformer_helper.py ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OpwqWL2QH5G"
   },
   "outputs": [],
   "source": [
    "# from os import environ\n",
    "# environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "# removes tensorflow warnings triggered because of Tensorflow incompatibility with my Apple M1 chip.\n",
    "# ignore this when using a non Apple Silicon device, ie. Google Colab or the likes.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, Dense, Input, Dropout, BatchNormalization\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6BKLL9B3vIZ"
   },
   "source": [
    "Cells Below is **only for TPUs**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WMA_zsLY3x6O"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')\n",
    "# tf.config.experimental_connect_to_cluster(resolver)\n",
    "# # This is the TPU initialization code that has to be at the beginning.\n",
    "# tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "# print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n",
    "\n",
    "# strategy = tf.distribute.TPUStrategy(resolver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0K8Ni6bD4Mge"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_DOA-JbhlT-4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "!pip install jupyterplot\n",
    "from jupyterplot import ProgressPlot as PP\n",
    "\n",
    "from transformer_helper import *\n",
    "from rolling_and_plot_tf import data_plot, rolling_split, normalize, validate\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvorie1ElT-5"
   },
   "source": [
    "Will have to figure out how to set device to cuda in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUteRx9dlT-5"
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Import](#0)\n",
    "- [JupyterPlot](#jup)\n",
    "- [Preprocessing](#win)\n",
    "- [Encoder](#enc)\n",
    "    - [Encoder Layer](#enc-lay)\n",
    "    - [Full Encoder](#full-enc)\n",
    "- [Transformer](#transform)\n",
    "- [Callbacks & Learn Rate Scheduler](#loss)\n",
    "- [Training](#train)\n",
    "- [Validate](#val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0EL21GdslT-5"
   },
   "source": [
    "# Literature:\n",
    "\n",
    "\n",
    "According to [A Transformer-based Framework for Multivariate Time Series Representation Learning](https://dl.acm.org/doi/abs/10.1145/3447548.3467401):\n",
    "Using **Batch Normalization is significantly more effective** for multivariate time-series than using the traditional Layer Normalization method found in NLP.\n",
    "\n",
    "In addition, according to [Deep learning approach towards accurate state of charge estimation for lithium-ion batteries using self-supervised transformer model](https://www.nature.com/articles/s41598-021-98915-8#Sec9):\n",
    "Using a transformer network while **forgoing the Decoder Layer** is more effective for the application of State-of-Charge estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG0gPyv0oDBi"
   },
   "source": [
    "$\\large{Self\\ Attention}$\n",
    "$$\n",
    "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2DSwSOZlT-7"
   },
   "source": [
    "$\\large{Input}$\n",
    "\n",
    "Voltage, Current, SOC at times:\n",
    "$$t - window\\_size - 1 \\rightarrow t - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bw-WpE1ulT-9"
   },
   "source": [
    "**Note**\n",
    "\n",
    "Cannot use embedding layers with battery data because of floating point values and negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WStD-7ytlT-9"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class G:\n",
    "    #preprocess\n",
    "    window_time = 96 #seconds\n",
    "    window_size = 32\n",
    "    slicing = window_time // window_size\n",
    "    batch_size = 16\n",
    "    #network\n",
    "    dense_dim = 32\n",
    "    model_dim = 128\n",
    "    num_features = 3 # current, voltage, and soc at t minus G.window_size -> t minus 1\n",
    "    num_heads = 16\n",
    "    num_layers = 6\n",
    "    #learning_rate_scheduler\n",
    "    T_i = 1\n",
    "    T_mult = 2\n",
    "    T_cur = 0.0\n",
    "    #training\n",
    "    epochs = 256 #should be a power of T_mult because of cosine annealing with warm restarts scheduler\n",
    "    learning_rate = 0.0045\n",
    "    min_learning_rate = 6e-11\n",
    "#     weight_decay = 0.0 #No weight decay param in the the keras optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prIueTe-lT-9"
   },
   "source": [
    "<a id=\"win\"></a>\n",
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "il6DI4Z7lT--"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "file = pd.read_csv(\"/Users/attar/Library/CloudStorage/OneDrive-UniversityofWaterloo/Simulated_Data/sim_data.csv\")\n",
    "#if using sim_data.csv:\n",
    "file[\"soc\"] *= 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SLQrFOvrlT--"
   },
   "outputs": [],
   "source": [
    "data_plot(data = [file],\n",
    "          title=\"OCV v SOC\",\n",
    "          x = [\"test time (sec)\"],\n",
    "          y = [\"soc\"],\n",
    "          markers = \"lines\",\n",
    "          color = \"darkorchid\",\n",
    "          x_title = \"Test Time (sec)\",\n",
    "          y_title = \"SOC\"\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_f7QighFlT--"
   },
   "outputs": [],
   "source": [
    "file = normalize(file.loc[:,[\"current\",\"voltage\",\"soc\"]].iloc[::G.slicing])\n",
    "#uses sklearn.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x79KvZ3ilT--"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = rolling_split(file, G.window_size, train=True)\n",
    "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
    "#uses sklearn.model_selection\n",
    "\n",
    "x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
    "x_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "y_test = tf.data.Dataset.from_tensor_slices(y_test)\n",
    "\n",
    "train_dataloader = tf.data.Dataset.zip((x_train, y_train)).batch(G.batch_size, drop_remainder=True)\n",
    "test_dataloader = tf.data.Dataset.zip((x_test, y_test)).batch(G.batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRmivoyVlT-_"
   },
   "outputs": [],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(f\"Shape of X [window, features]: {x.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blS0pEpTqRVI"
   },
   "source": [
    "<a name='enc'></a>\n",
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sC5vJhz29vZR"
   },
   "outputs": [],
   "source": [
    "def FullyConnected():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(G.dense_dim, activation='relu',\n",
    "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.005, maxval = 0.08)\n",
    "                             ),\n",
    "        # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        tf.keras.layers.BatchNormalization(momentum = 0.98, epsilon=5e-4),\n",
    "        tf.keras.layers.Dense(G.dense_dim, activation='relu',\n",
    "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                             ),\n",
    "        # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        tf.keras.layers.BatchNormalization(momentum = 0.95, epsilon=5e-4)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R65WbX5wqYYH"
   },
   "source": [
    "<a name='enc-lay'></a>\n",
    "###  Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIufbrc-9_2u"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
    "    followed by a simple, positionwise fully connected feed-forward network. \n",
    "    This archirecture includes a residual connection around each of the two \n",
    "    sub-layers, followed by batch normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_heads,\n",
    "                 num_features,\n",
    "                 dense_dim,\n",
    "                 dropout_rate,\n",
    "                 batchnorm_eps):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(\n",
    "            num_heads = num_heads,\n",
    "            key_dim = dense_dim,\n",
    "            dropout = dropout_rate,\n",
    "            kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "            # kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
    "            bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
    "                                     )\n",
    "        \n",
    "        #feed-forward-network\n",
    "        self.ffn = FullyConnected()\n",
    "        \n",
    "        \n",
    "        self.batchnorm1 = BatchNormalization(momentum = 0.95, epsilon=batchnorm_eps)\n",
    "        self.batchnorm2 = BatchNormalization(momentum = 0.95, epsilon=batchnorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder Layer\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "        Returns:\n",
    "            encoder_layer_out -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "        \"\"\"\n",
    "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
    "        \n",
    "        attn_output = self.mha(query = x,\n",
    "                               value = x) # Self attention\n",
    "        \n",
    "        out1 = self.batchnorm1(tf.add(x, attn_output))  # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "    \n",
    "        ffn_output = self.dropout_ffn(ffn_output) # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        \n",
    "        encoder_layer_out = self.batchnorm2(tf.add(ffn_output, out1))\n",
    "        # (G.batch_size, G.window_size, G.dense_dim)\n",
    "        return encoder_layer_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKgObFUUlT_B"
   },
   "source": [
    "<a name='full-enc'></a>\n",
    "### Full Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7j2Tjr0K0t0I"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    The entire Encoder starts by passing the input to an embedding layer \n",
    "    and using positional encoding to then pass the output through a stack of\n",
    "    encoder Layers\n",
    "        \n",
    "    \"\"\"  \n",
    "    def __init__(self,\n",
    "                 num_layers = G.num_layers,\n",
    "                 num_heads = G.num_heads,\n",
    "                 num_features = G.num_features,\n",
    "                 dense_dim = G.dense_dim,\n",
    "                 maximum_position_encoding = G.window_size,\n",
    "                 dropout_rate=0.15,\n",
    "                 batchnorm_eps=1e-4):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        #linear input layer\n",
    "        self.lin_input = tf.keras.layers.Dense(dense_dim, activation=\"relu\")\n",
    "        \n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                dense_dim)\n",
    "\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(num_heads = num_heads,\n",
    "                                        num_features = num_features,\n",
    "                                        dense_dim = dense_dim,\n",
    "                                        dropout_rate = dropout_rate,\n",
    "                                        batchnorm_eps = batchnorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the Encoder\n",
    "        \n",
    "        Arguments:\n",
    "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout layers\n",
    "            mask -- Boolean mask to ensure that the padding is not \n",
    "                    treated as part of the input\n",
    "        Returns:\n",
    "            Tensor of shape (G.batch_size, G.window_size, G.dense_dim)\n",
    "        \"\"\"\n",
    "        x = self.lin_input(x)\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training)\n",
    "            \n",
    "        # only need the final time's data : time = t-1 from the window\n",
    "        # x has shape (G.batch_size, G.window_size, G.dense_dim)\n",
    "        # but I am only returning time t-1:\n",
    "        return x[:, -1, :] # (G.batch_size, G.dense_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U2F58rnlT_C"
   },
   "source": [
    "<a name='transform'></a> \n",
    "# Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QHymPmaj-2ba"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Complete transformer with an Encoder and a Decoder\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 num_layers = G.num_layers,\n",
    "                 num_heads = G.num_heads,\n",
    "                 dense_dim = G.dense_dim,\n",
    "                 max_positional_encoding_input = G.window_size,\n",
    "                 max_positional_encoding_target = G.window_size):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "\n",
    "        self.encoder = Encoder()\n",
    "\n",
    "        self.final_stack = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(\n",
    "                dense_dim, activation = \"relu\",\n",
    "                kernel_initializer = tf.keras.initializers.HeNormal(),\n",
    "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.02)\n",
    "                                  ),\n",
    "            tf.keras.layers.BatchNormalization(momentum = 0.97, epsilon=5e-4),\n",
    "\n",
    "            tf.keras.layers.Dense(\n",
    "                1, activation = \"sigmoid\",\n",
    "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.005)\n",
    "                                 )\n",
    "                                              ])\n",
    "    \n",
    "    def call(self, x, training):\n",
    "        \"\"\"\n",
    "        Forward pass for the entire Transformer\n",
    "        Arguments:\n",
    "            x -- tf.data.Dataset containing batch inputs and targets\n",
    "                 batched & windowed voltage, current and soc data with batched soc targets\n",
    "            training -- Boolean, set to true to activate\n",
    "                        the training mode for dropout and batchnorm layers\n",
    "        Returns:\n",
    "            final_output -- SOC prediction at time t\n",
    "        \n",
    "        \"\"\"\n",
    "        enc_output = self.encoder(x, training) # (G.batch_size, G.dense_dim)\n",
    "        \n",
    "        final_output = self.final_stack(enc_output) # (G.batch_size, 1)\n",
    "\n",
    "\n",
    "    \n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiILRshLv9Bx"
   },
   "source": [
    "## Note:\n",
    "\n",
    "The `training` argument in the model and layer calls sets the `keras.backend.learning_phase()` value to the appropriate value for the use case.\n",
    "ie.\n",
    "- If I am using the train_loop(), `training` is set to True which means all the Dropout and BatchNormalization layers are active.\n",
    "- If I am using the test_loop(), `training` is set to False which means all the Dropout and BatchNormalization layers are inactive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6IncgGX4z_9"
   },
   "source": [
    "If Using **TPUs** use the cell right below this text\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un5xiWL644Uf"
   },
   "outputs": [],
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# with strategy.scope():\n",
    "#     model = Transformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeCjW7VP44fP"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ8bVUEh45Mj"
   },
   "source": [
    "If **not using TPUs**:\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ovllyglWlT_C"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = Transformer()\n",
    "model.build((G.batch_size, G.window_size, G.num_features))\n",
    "model.summary(expand_nested=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWtYX-8348Z1"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JUcLoUmWlT_D"
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yYtQv1TtlT_D"
   },
   "source": [
    "<a id = \"loss\"></a>\n",
    "# Callbacks and Scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTN3TiSblT_D"
   },
   "source": [
    "**Learning Rate Scheduler**\n",
    "\n",
    "Cosine Annealing with Warm Restarts proposed by Loshchilov et al. in [SGDR: Stochastic Gradient Descent with Warm Restarts](https://doi.org/10.48550/arXiv.1608.03983)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWt1eUd9o6WA"
   },
   "source": [
    "$$\\mu_t = \\mu_{min} + \\frac{1}{2}(\\mu_{max} - \\mu_{min})\\cdot (1 + \\cos (\\frac{T_{cur}}{T_i}\\pi))$$\n",
    "\n",
    "Where:\n",
    " - $\\mu$ is the learning_rate, subscript $t$ is for time = $t$\n",
    " - $T_{cur}$ is the number of epochs since the last restart\n",
    " - $T_i$ is the number of epochs between two restarts\n",
    "\n",
    "Note:\n",
    " - When $T_{cur} = T_i \\rightarrow \\mu_t = \\mu_{min}$\n",
    " - When $T_{cur} = 0 \\rightarrow \\mu_t = \\mu_{max}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sLjZ7ICoSGif"
   },
   "source": [
    "---\n",
    "**The Cell below is for the LambdaCallback Class in keras in order to implement Cosine Annealing with Warm Restarts** â†“\n",
    "\n",
    "Used with callbacks in model.fit()\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZg1uSmDQMTZ"
   },
   "outputs": [],
   "source": [
    "def schedule(batch, logs):\n",
    "        '''\n",
    "        This is a dummy function for the LearningRateScheduler Class\n",
    "        I am trying to see if I can use the model.compile(), model.fit(), model.evaluate(), trio with\n",
    "        Cosine Annealing with Warm Restarts\n",
    "        Returns a new learning rate based on the schedule described below\n",
    "        \n",
    "        Call after every batch\n",
    "        '''\n",
    "        \n",
    "        mu_i = G.min_learning_rate + 0.5 * (\n",
    "                G.learning_rate - G.min_learning_rate) * (\n",
    "                    1 + tf.math.cos(np.pi * G.T_cur / G.T_i))\n",
    "        \n",
    "        G.T_cur += G.batch_size / len(x_train)\n",
    "        if np.isclose(G.T_cur, G.T_i):\n",
    "            G.T_i *= G.T_mult\n",
    "            G.T_cur = 0.0\n",
    "        K.set_value(model.optimizer.learning_rate, mu_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzZcCFve2o5O"
   },
   "source": [
    "**Progress Plot Callback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZeomH0iN2o5O"
   },
   "outputs": [],
   "source": [
    "class ProgressCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        train_loss = logs[\"loss\"]\n",
    "        train_acc = 100.0 - logs[\"mean_absolute_percentage_error\"]\n",
    "        test_loss = logs[\"val_loss\"]\n",
    "        test_acc = 100.0 - logs[\"val_mean_absolute_percentage_error\"]\n",
    "        global pp\n",
    "        pp.update([[train_loss, test_loss],\n",
    "                   [train_acc, test_acc]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A699g9Sp2o5P"
   },
   "source": [
    "**Save Model Progress Callback**\n",
    "\n",
    "Does not work with TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTdfb-br2o5P"
   },
   "outputs": [],
   "source": [
    "class SaveModel(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs = None):\n",
    "        if epoch != 0 and epoch % 15 == 0:\n",
    "            self.model.save_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mgkzjt8NReWS"
   },
   "source": [
    "**Early Stopping and Saving Best Model checkpoint Callbacks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_-Lh-49NRb_r"
   },
   "outputs": [],
   "source": [
    "model_options = tf.saved_model.SaveOptions(experimental_io_device=\"/job:localhost\")\n",
    "# earlystopping = EarlyStopping(monitor='val_mean_absolute_percentage_error', patience=150, verbose=0, mode='min')\n",
    "mcp_save = ModelCheckpoint('/content/drive/MyDrive/transformer_soc/tpu_model_weights', save_format = \"tf\", save_best_only=True, monitor='val_mean_absolute_percentage_error', mode='min', options = model_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hg7FmZOHlT_E"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.LogCosh()\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = G.learning_rate,\n",
    "                                     beta_1 = 0.9,\n",
    "                                     beta_2 = 0.999\n",
    "                                    )\n",
    "\n",
    "#cos_anneal is for the model.fit() call\n",
    "cos_anneal = tf.keras.callbacks.LambdaCallback(on_batch_end = schedule)\n",
    "\n",
    "#progress plot callback\n",
    "pp_update = ProgressCallback()\n",
    "\n",
    "#model parameters save callback\n",
    "model_save = SaveModel() #This is optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45--3qknlT_H"
   },
   "source": [
    "<a id = \"train\"></a>\n",
    "# Training\n",
    "\n",
    "**There are two compile calls, one requires a TPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ynnk3or6-FMd"
   },
   "outputs": [],
   "source": [
    "pp = PP(plot_names = [\"Mean Log Loss\", \"% Accuracy\"],\n",
    "        line_names = [\"Train Loop\", \"Test Loop\"],\n",
    "        x_label = \"epochs\"\n",
    "       )\n",
    "\n",
    "# ##### if using a TPU:\n",
    "# with strategy.scope():\n",
    "#     model.compile(optimizer, loss_object, steps_per_execution = 3, metrics=[\"mean_absolute_percentage_error\"])\n",
    "\n",
    "##### else:\n",
    "# model.compile(optimizer, loss_object, metrics=[\"mean_absolute_percentage_error\"])\n",
    "## Dont compile after training, it causes issues.\n",
    "\n",
    "#-----------------------------------------------------------------\n",
    "#Note: can add `model_save` to the callbacks list in model.fit()\n",
    "#      it saves the model params to the google drive every 15 epochs\n",
    "#-------------------------------------------------------------------\n",
    "\n",
    "steps_per_epoch = len(train_dataloader) // G.epochs\n",
    "validation_steps = len(test_dataloader) // G.epochs\n",
    "\n",
    "history = model.fit(train_dataloader,\n",
    "                    batch_size = G.batch_size,\n",
    "                    epochs = G.epochs,\n",
    "                    verbose = 1,\n",
    "                    steps_per_epoch = steps_per_epoch,\n",
    "                    callbacks = [cos_anneal, pp_update],\n",
    "                    validation_data = test_dataloader,\n",
    "                    validation_steps = validation_steps\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yF6RygxlT_I"
   },
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/transformer_soc/tpu_model.h5\") #doesnt work with TPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljBF-U_vIjrL"
   },
   "outputs": [],
   "source": [
    "#works with TPUs\n",
    "checkpoint = tf.train.Checkpoint(model = model)\n",
    "options = tf.train.CheckpointOptions(experimental_io_device=\"/job:localhost\")\n",
    "checkpoint.save(\"/content/drive/MyDrive/transformer_soc/tpu_model/ckpt\", options=options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5pSwH7QlT_I"
   },
   "source": [
    "<a id = \"val\"></a>\n",
    "# Validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYr2y9eulT_I"
   },
   "source": [
    "**Dev Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CuY9saCblT_I",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualize_dev = validate(model, test_dataloader, dev = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5uLkWkLlT_I"
   },
   "source": [
    "**Entire Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjvsvbIllT_I"
   },
   "outputs": [],
   "source": [
    "x_set, y_set = rolling_split(file, G.window_size, train = False)\n",
    "\n",
    "x_set = tf.data.Dataset.from_tensor_slices(x_set)\n",
    "y_set = tf.data.Dataset.from_tensor_slices(y_set)\n",
    "\n",
    "set_dataloader = tf.data.Dataset.zip((x_set, y_set)).batch(G.batch_size, drop_remainder=True)\n",
    "\n",
    "visualize = validate(model, set_dataloader, dev = False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "transform_notebook.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
