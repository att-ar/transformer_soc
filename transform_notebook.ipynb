{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_Iw1qCBlT-z"
      },
      "source": [
        "<a name='0'></a>\n",
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqXowf9MlT-1"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eh1JdQmwlT-3"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/transformer_soc/rolling_and_plot_tf.py .\n",
        "!cp /content/drive/MyDrive/transformer_soc/sim_data.csv .\n",
        "!cp /content/drive/MyDrive/transformer_soc/transformer_helper.py ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OpwqWL2QH5G"
      },
      "outputs": [],
      "source": [
        "# from os import environ\n",
        "# environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
        "# removes tensorflow warnings triggered because of Tensorflow incompatibility with my Apple M1 chip.\n",
        "# ignore this when using a non Apple Silicon device, ie. Google Colab or the likes.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import MultiHeadAttention, Dense, Input, Dropout, BatchNormalization\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "from sklearn.preprocessing import MaxAbsScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DOA-JbhlT-4"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "!pip install jupyterplot\n",
        "from jupyterplot import ProgressPlot as PP\n",
        "\n",
        "from transformer_helper import *\n",
        "from rolling_and_plot_tf import data_plot, rolling_split, normalize, validate\n",
        "\n",
        "%reload_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvorie1ElT-5"
      },
      "source": [
        "Will have to figure out how to set device to cuda in TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUteRx9dlT-5"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "- [Import](#0)\n",
        "- [JupyterPlot](#jup)\n",
        "- [Preprocessing](#win)\n",
        "- [Encoder](#enc)\n",
        "    - [Encoder Layer](#enc-lay)\n",
        "    - [Full Encoder](#full-enc)\n",
        "- [Transformer](#transform)\n",
        "- [Loss and Learn Rate Scheduler](#loss)\n",
        "- [Training](#train)\n",
        "- [Validate](#val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EL21GdslT-5"
      },
      "source": [
        "# Literature:\n",
        "\n",
        "\n",
        "According to [A Transformer-based Framework for Multivariate Time Series Representation Learning](https://dl.acm.org/doi/abs/10.1145/3447548.3467401):\n",
        "Using **Batch Normalization is significantly more effective** for multivariate time-series than using the traditional Layer Normalization method found in NLP.\n",
        "\n",
        "In addition, according to [Deep learning approach towards accurate state of charge estimation for lithium-ion batteries using self-supervised transformer model](https://www.nature.com/articles/s41598-021-98915-8#Sec9):\n",
        "Using a transformer network while **forgoing the Decoder Layer** is more effective for the application of State-of-Charge estimation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VG0gPyv0oDBi"
      },
      "source": [
        "$\\large{Self\\ Attention}$\n",
        "$$\n",
        "\\text { Attention }(Q, K, V)=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}+{M}\\right) V\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2DSwSOZlT-7"
      },
      "source": [
        "$\\large{Input}$\n",
        "\n",
        "Voltage, Current, SOC at times:\n",
        "$$t - window\\_size - 1 \\rightarrow t - 1 $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw-WpE1ulT-9"
      },
      "source": [
        "**Note**\n",
        "\n",
        "Cannot use embedding layers with battery data because of floating point values and negative values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WStD-7ytlT-9"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class G:\n",
        "    #preprocess\n",
        "    window_time = 96 #seconds\n",
        "    window_size = 32\n",
        "    slicing = window_time // window_size\n",
        "    batch_size = 16\n",
        "    #network\n",
        "    dense_dim = 32\n",
        "    model_dim = 128\n",
        "    num_features = 3 # current, voltage, and soc at t minus G.window_size -> t minus 1\n",
        "    num_heads = 16\n",
        "    num_layers = 6\n",
        "    #learning_rate_scheduler\n",
        "    T_i = 1\n",
        "    T_mult = 3\n",
        "    T_cur = 0.0\n",
        "    #training\n",
        "    epochs = 100\n",
        "    learning_rate = 0.0035\n",
        "    min_learning_rate = 7e-11\n",
        "#     weight_decay = 0.0 #No weight decay param in the the keras optimizers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prIueTe-lT-9"
      },
      "source": [
        "<a id=\"win\"></a>\n",
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "il6DI4Z7lT--"
      },
      "outputs": [],
      "source": [
        "# from google.colab import files\n",
        "file = pd.read_csv(\"/content/sim_data.csv\")\n",
        "#if using sim_data.csv:\n",
        "file[\"soc\"] *= 100.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLQrFOvrlT--"
      },
      "outputs": [],
      "source": [
        "data_plot(data = [file],\n",
        "          title=\"OCV v SOC\",\n",
        "          x = [\"test time (sec)\"],\n",
        "          y = [\"soc\"],\n",
        "          markers = \"lines\",\n",
        "          color = \"darkorchid\",\n",
        "          x_title = \"Test Time (sec)\",\n",
        "          y_title = \"SOC\"\n",
        "         )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_f7QighFlT--"
      },
      "outputs": [],
      "source": [
        "file = normalize(file.loc[:,[\"current\",\"voltage\",\"soc\"]].iloc[::G.slicing])\n",
        "#uses sklearn.preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x79KvZ3ilT--"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = rolling_split(file, G.window_size, train=True)\n",
        "print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)\n",
        "#uses sklearn.model_selection\n",
        "\n",
        "x_train = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "y_train = tf.data.Dataset.from_tensor_slices(y_train)\n",
        "x_test = tf.data.Dataset.from_tensor_slices(x_test)\n",
        "y_test = tf.data.Dataset.from_tensor_slices(y_test)\n",
        "\n",
        "train_dataloader = tf.data.Dataset.zip((x_train, y_train)).batch(G.batch_size, drop_remainder=True)\n",
        "test_dataloader = tf.data.Dataset.zip((x_test, y_test)).batch(G.batch_size, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRmivoyVlT-_"
      },
      "outputs": [],
      "source": [
        "for x,y in train_dataloader:\n",
        "    print(f\"Shape of X [window, features]: {x.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blS0pEpTqRVI"
      },
      "source": [
        "<a name='enc'></a>\n",
        "# Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sC5vJhz29vZR"
      },
      "outputs": [],
      "source": [
        "def FullyConnected():\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(G.dense_dim, activation='relu',\n",
        "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.005, maxval = 0.08)\n",
        "                             ),\n",
        "        # (G.batch_size, G.window_size, G.dense_dim)\n",
        "        tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4),\n",
        "        tf.keras.layers.Dense(G.num_features, activation='relu',\n",
        "                              kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                              bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
        "                             ),\n",
        "        # (G.batch_size, G.window_size, G.num_features)\n",
        "        tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4)\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R65WbX5wqYYH"
      },
      "source": [
        "<a name='enc-lay'></a>\n",
        "###  Encoder Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIufbrc-9_2u"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The encoder layer is composed by a multi-head self-attention mechanism,\n",
        "    followed by a simple, positionwise fully connected feed-forward network. \n",
        "    This archirecture includes a residual connection around each of the two \n",
        "    sub-layers, followed by batch normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_heads,\n",
        "                 num_features,\n",
        "                 dense_dim,\n",
        "                 dropout_rate=0.35,\n",
        "                 batchnorm_eps=1e-3):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(\n",
        "            num_heads = num_heads,\n",
        "            key_dim = num_features,\n",
        "            dropout = dropout_rate,\n",
        "            kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "            kernel_regularizer = tf.keras.regularizers.L2(1e-4),\n",
        "            bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.01)\n",
        "                                     )\n",
        "        #feed-forward-network\n",
        "        self.ffn = FullyConnected()\n",
        "        \n",
        "        \n",
        "        self.batchnorm1 = BatchNormalization(momentum = 0.9, epsilon=batchnorm_eps)\n",
        "        self.batchnorm2 = BatchNormalization(momentum = 0.85, epsilon=batchnorm_eps)\n",
        "\n",
        "        self.dropout_ffn = Dropout(dropout_rate)\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder Layer\n",
        "        \n",
        "        Arguments:\n",
        "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "        Returns:\n",
        "            encoder_layer_out -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
        "        \"\"\"\n",
        "        # Dropout is added by Keras automatically if the dropout parameter is non-zero during training\n",
        "        attn_output = self.mha(query = x,\n",
        "                               value = x) # Self attention\n",
        "        \n",
        "        out1 = self.batchnorm1(tf.add(x, attn_output))  # (G.batch_size, G.window_size, G.num_features)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "    \n",
        "        ffn_output = self.dropout_ffn(ffn_output) # (G.batch_size, G.window_size, G.num_features)\n",
        "        \n",
        "        encoder_layer_out = self.batchnorm2(tf.add(ffn_output, out1))\n",
        "        # (G.batch_size, G.window_size, G.num_features)\n",
        "        return encoder_layer_out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKgObFUUlT_B"
      },
      "source": [
        "<a name='full-enc'></a>\n",
        "### Full Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j2Tjr0K0t0I"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    The entire Encoder starts by passing the input to an embedding layer \n",
        "    and using positional encoding to then pass the output through a stack of\n",
        "    encoder Layers\n",
        "        \n",
        "    \"\"\"  \n",
        "    def __init__(self,\n",
        "                 num_layers = G.num_layers,\n",
        "                 num_heads = G.num_heads,\n",
        "                 num_features = G.num_features,\n",
        "                 dense_dim = G.dense_dim,\n",
        "                 input_size = G.num_features,\n",
        "                 maximum_position_encoding = G.window_size,\n",
        "                 dropout_rate=0.35,\n",
        "                 batchnorm_eps=1e-6):\n",
        "        \n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "#         self.embedding_dim = embedding_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "#         self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, \n",
        "                                                input_size)\n",
        "\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(num_heads = num_heads,\n",
        "                                        num_features = num_features,\n",
        "                                        dense_dim = dense_dim,\n",
        "                                        dropout_rate = dropout_rate,\n",
        "                                        batchnorm_eps = batchnorm_eps) \n",
        "                           for _ in range(self.num_layers)]\n",
        "\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        \n",
        "    def call(self, x, training):\n",
        "        \"\"\"\n",
        "        Forward pass for the Encoder\n",
        "        \n",
        "        Arguments:\n",
        "            x -- Tensor of shape (G.batch_size, G.window_size, G.num_features)\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "            mask -- Boolean mask to ensure that the padding is not \n",
        "                    treated as part of the input\n",
        "        Returns:\n",
        "            out2 -- Tensor of shape (G.batch_size, G.window_size, G.dense_dim)\n",
        "        \"\"\"\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        \n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x,training)\n",
        "            \n",
        "        # only need the final time's data : time = t-1 from the window\n",
        "        # x has shape (G.batch_size, G.window_size, G.dense_dim)\n",
        "        # but I am only returning time t-1:\n",
        "        return x[:, -1, :] # (G.batch_size, G.dense_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U2F58rnlT_C"
      },
      "source": [
        "<a name='transform'></a> \n",
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHymPmaj-2ba"
      },
      "outputs": [],
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    Complete transformer with an Encoder and a Decoder\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 num_layers = G.num_layers,\n",
        "                 num_heads = G.num_heads,\n",
        "                 dense_dim = G.dense_dim,\n",
        "                 max_positional_encoding_input = G.window_size,\n",
        "                 max_positional_encoding_target = G.window_size):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "\n",
        "        self.encoder = Encoder()\n",
        "\n",
        "        self.final_stack = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(\n",
        "                dense_dim, activation = \"relu\",\n",
        "                kernel_initializer = tf.keras.initializers.HeNormal(),\n",
        "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.02)\n",
        "                                  ),\n",
        "            tf.keras.layers.BatchNormalization(momentum = 0.90, epsilon=5e-4),\n",
        "\n",
        "            tf.keras.layers.Dense(\n",
        "                1, activation = \"sigmoid\",\n",
        "                bias_initializer = tf.keras.initializers.RandomUniform(minval=0.001, maxval = 0.005)\n",
        "                                 )\n",
        "                                              ])\n",
        "    \n",
        "    def call(self, x, training):\n",
        "        \"\"\"\n",
        "        Forward pass for the entire Transformer\n",
        "        Arguments:\n",
        "            input_data -- Tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
        "                              An array of the windowed voltage, current and time data\n",
        "            output_soc -- Tensor of shape (batch_size, target_seq_len, fully_connected_dim)\n",
        "                              An array of the indexes of the words in the output sentence\n",
        "            training -- Boolean, set to true to activate\n",
        "                        the training mode for dropout layers\n",
        "        Returns:\n",
        "            final_output -- SOC prediction at time t\n",
        "        \n",
        "        \"\"\"\n",
        "        enc_output = self.encoder(x, training) # (G.batch_size, G.dense_dim)\n",
        "        \n",
        "        final_output = self.final_stack(enc_output) # (G.batch_size, 1)\n",
        "\n",
        "\n",
        "    \n",
        "        return final_output"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Note:\n",
        "\n",
        "The `training` argument in the model and layer calls sets the `keras.backend.learning_phase()` value to the appropriate value for the use case.\n",
        "ie.\n",
        "- If I am using the train_loop(), `training` is set to True which means all the Dropout and BatchNormalization layers are active.\n",
        "- If I am using the test_loop(), `training` is set to False which means all the Dropout and BatchNormalization layers are inactive."
      ],
      "metadata": {
        "id": "kiILRshLv9Bx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovllyglWlT_C"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "model = Transformer()\n",
        "model.build((G.batch_size, G.window_size, G.num_features))\n",
        "model.summary(expand_nested=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUcLoUmWlT_D"
      },
      "outputs": [],
      "source": [
        "model.load_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.tf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYtQv1TtlT_D"
      },
      "source": [
        "<a id = \"loss\"></a>\n",
        "# Loss and LR Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-3MIaQslT_D"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, x, y, training):\n",
        "    y_hat = model(x, training=training)\n",
        "    return y_hat, loss_object(y_true = y, y_pred = y_hat)\n",
        "\n",
        "def grad(model, inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "        y_hat, loss = loss_fn(model, inputs, targets, training=True)\n",
        "    return y_hat, loss, tape.gradient(loss, model.trainable_variables)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTN3TiSblT_D"
      },
      "source": [
        "**Learning Rate Scheduler**\n",
        "\n",
        "Cosine Annealing with Warm Restarts proposed by Loshchilov et al. in [SGDR: Stochastic Gradient Descent with Warm Restarts](https://doi.org/10.48550/arXiv.1608.03983)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\mu_t = \\mu_{min} + \\frac{1}{2}(\\mu_{max} - \\mu_{min})\\cdot (1 + \\cos (\\frac{T_{cur}}{T_i}\\pi))$$\n",
        "\n",
        "Where:\n",
        " - $\\mu$ is the learning_rate, subscript $t$ is for time = $t$\n",
        " - $T_{cur}$ is the number of epochs since the last restart\n",
        " - $T_i$ is the number of epochs between two restarts\n",
        "\n",
        "Note:\n",
        " - When $T_{cur} = T_i \\rightarrow \\mu_t = \\mu_{min}$\n",
        " - When $T_{cur} = 0 \\rightarrow \\mu_t = \\mu_{max}$"
      ],
      "metadata": {
        "id": "xWt1eUd9o6WA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**The Cell below is for the LambdaCallback Class in keras in order to implement Cosine Annealing with Warm Restarts** ↓\n",
        "\n",
        "Used with callbacks in model.fit()<br>\n",
        "For training via the Tensorflow norm\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "sLjZ7ICoSGif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def schedule(batch, logs):\n",
        "        '''\n",
        "        This is a dummy function for the LearningRateScheduler Class\n",
        "        I am trying to see if I can use the model.compile(), model.fit(), model.evaluate(), trio with CosineAnnealingWR\n",
        "        Cosine Annealing with Warm Restarts\n",
        "        Returns a new learning rate based on the schedule described below\n",
        "        \n",
        "        Call after every batch\n",
        "        '''\n",
        "        \n",
        "        mu_i = G.min_learning_rate + 0.5 * (\n",
        "                G.learning_rate - G.min_learning_rate) * (\n",
        "                    1 + tf.math.cos(np.pi * G.T_cur / G.T_i))\n",
        "        \n",
        "        G.T_cur += G.batch_size / len(x_train)\n",
        "        if np.isclose(G.T_cur, G.T_i):\n",
        "            G.T_i *= G.T_mult\n",
        "            G.T_cur = 0.0\n",
        "        K.set_value(model.optimizer.learning_rate, mu_i)"
      ],
      "metadata": {
        "id": "mZg1uSmDQMTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**The Cell below is my own CosineAnnealing with WarmRestarts class** ↓\n",
        "\n",
        "Used with the train_loop() and test_loop() functions.<br>\n",
        "For training via my own custom functions.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "QdkMbo7ASQr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ed637jTilT_D"
      },
      "outputs": [],
      "source": [
        "class CosAnnealWarmRestarts():\n",
        "    def __init__(self, T_0: float, T_mult: float):\n",
        "        '''\n",
        "        Cosine Annealing with Warm Restarts\n",
        "        Returns a new learning rate based on the call method\n",
        "        \n",
        "        Parameters:\n",
        "        `T_0` int\n",
        "            the number of iterations for the first restart to occur\n",
        "        `T_mult` int\n",
        "            the factor to increase T_i by after a restart, where T_i is the i^th restart.\n",
        "        '''\n",
        "        super(CosAnnealWarmRestarts, self).__init__()\n",
        "        \n",
        "        assert isinstance(T_0, float) and isinstance(T_mult, float)\n",
        "        assert T_0 > 0.0\n",
        "        self.mu_max = G.learning_rate #initial and max learning_rate\n",
        "        self.mu_min = G.min_learning_rate #minimum learning_rate\n",
        "        self.T_i = T_0\n",
        "        self.T_mult = T_mult\n",
        "        self.T_cur = 0.0\n",
        "        \n",
        "        self.learning_rate = G.learning_rate\n",
        "        \n",
        "    def step(self, increment:float, optimizer):\n",
        "        '''\n",
        "        Cosine Annealing with Warm Restarts\n",
        "        Returns a new learning rate based on the schedule described below\n",
        "        \n",
        "        Call after every batch\n",
        "\n",
        "        Parameters:\n",
        "        `increment` float\n",
        "            1 batch / total number of batches\n",
        "            !!!!! Not the current batch number, that would be a series summation\n",
        "            every epoch, a total of 1.0 will be added to self.T_cur\n",
        "        `optimizer`\n",
        "            the optimizer for the neural network\n",
        "        '''\n",
        "        try:\n",
        "            optimizer.learning_rate\n",
        "        except AttributeError:\n",
        "            print(\"Error: optimizer does not have a learning_rate parameter\")\n",
        "        \n",
        "        mu_i = self.mu_min + 0.5 * (\n",
        "                self.mu_max - self.mu_min) * (\n",
        "                    1 + tf.math.cos(np.pi * self.T_cur / self.T_i))\n",
        "        \n",
        "        self.T_cur += increment\n",
        "        \n",
        "        if np.isclose(self.T_cur, self.T_i):\n",
        "            self.T_i *= self.T_mult\n",
        "            self.T_cur = 0.0\n",
        "        \n",
        "        #update the learning_rate accordingly:\n",
        "        optimizer.learning_rate.assign(tf.cast(mu_i,tf.float32))\n",
        "        \n",
        "        self.learning_rate = mu_i\n",
        "        #this is just so that you can find the current learning rate from the scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg7FmZOHlT_E"
      },
      "outputs": [],
      "source": [
        "loss_object = tf.keras.losses.LogCosh()\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate = G.learning_rate,\n",
        "                                     beta_1 = 0.9,\n",
        "                                     beta_2 = 0.999\n",
        "                                    )\n",
        "#scheduler is for my train_loop() function\n",
        "# scheduler = CosAnnealWarmRestarts(T_0 = 1.0, T_mult = 3.0)\n",
        "#cos_anneal is for the model.fit() call\n",
        "cos_anneal = tf.keras.callbacks.LambdaCallback(on_batch_end = schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut8epLdhlT_E"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer, scheduler, training = True):\n",
        "    size = len(dataloader)\n",
        "    perc_error = 0.0\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    \n",
        "    for batch, (x,y) in enumerate(dataloader):\n",
        "        \n",
        "        predict, loss, grads = grad(model, x, y) # assert(loss.shape == [])\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "        scheduler.step(1 / size, optimizer)\n",
        "        \n",
        "        perc_error += tf.math.reduce_mean(tf.abs(predict - y) / (y + 1e-2) * 100, [0,1])\n",
        "        epoch_loss_avg.update_state(loss)\n",
        "        if batch % (size // 15) == 0:\n",
        "            print(f\"Mean loss: {epoch_loss_avg.result():>7f}  [{batch:4d}/{size:4d}]\")\n",
        "\n",
        "    perc_error /= size\n",
        "    print(f\"Train Error: \\nAverage Accuracy: {100 - perc_error}%\")\n",
        "    return epoch_loss_avg.result(), 100. -perc_error\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn, training = False):\n",
        "    print(model.in_train_phase())\n",
        "    size = len(dataloader)\n",
        "    perc_error = 0.0\n",
        "    counter = 0\n",
        "    epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "    \n",
        "    for x,y in dataloader:\n",
        "        predict, test_loss = loss_fn(model, x, y, training)\n",
        "        \n",
        "        if np.isnan(test_loss).any():\n",
        "            print(\"Test Loss had a np.nan value\")\n",
        "            break\n",
        "        \n",
        "        epoch_loss_avg.update_state(test_loss)\n",
        "        perc_error += tf.math.reduce_mean(tf.abs(predict - y) / (y + 1e-2) * 100, [0,1])\n",
        "\n",
        "        counter += 1\n",
        "        if counter % (size // 2) == 0:\n",
        "            print(f\"{counter} / {size} tested\")\n",
        "            \n",
        "    perc_error /= size\n",
        "    print(f\"Test Error: \\nAverage Accuracy: {100 - perc_error}%, Avg Loss: {epoch_loss_avg.result():>8f}\\n\")\n",
        "    return epoch_loss_avg.result(), 100. - perc_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45--3qknlT_H"
      },
      "source": [
        "<a id = \"train\"></a>\n",
        "# Training\n",
        "\n",
        "**Just want to clarify:**\n",
        "\n",
        "The two training methods should be doing the same thing. I kept my own method available because of the rolling plot feature"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Cell below does not use my own functions and scheduler class\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "prYZhFhYUnHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer, loss_object,metrics=[\"mean_absolute_percentage_error\"])\n",
        "history = model.fit(train_dataloader,\n",
        "                    batch_size = G.batch_size,\n",
        "                    epochs = 100, verbose = 1,\n",
        "                    callbacks=[cos_anneal],\n",
        "                    validation_data=(test_dataloader))"
      ],
      "metadata": {
        "id": "ynnk3or6-FMd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Cell below uses my own functions and scheduler class\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "f0HjNixRUwHO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3NZQjtqClT_H"
      },
      "outputs": [],
      "source": [
        "pp = PP(plot_names = [\"Mean Log Loss\", \"% Accuracy\"],\n",
        "        line_names = [\"Train Loop\", \"Test Loop\"],\n",
        "        x_label = \"epochs\"\n",
        "       )\n",
        "\n",
        "for epoch in range(1, G.epochs+1):\n",
        "    print(f\"Epoch {epoch}/{G.epochs}\\n--------------------------------------\")\n",
        "    train_loss, train_acc = train_loop(train_dataloader, model, loss_fn, optimizer, scheduler)\n",
        "    test_loss, test_acc = test_loop(test_dataloader, model, loss_fn)\n",
        "    pp.update([[train_loss.numpy(), test_loss.numpy()], [train_acc, test_acc]])\n",
        "    \n",
        "    # if epoch % 15:\n",
        "    #     model.save_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.tf\", overwrite = True)\n",
        "    \n",
        "print(\"Completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yF6RygxlT_I"
      },
      "outputs": [],
      "source": [
        "model.save_weights(\"/content/drive/MyDrive/transformer_soc/model_weights.tf\", overwrite = True)\n",
        "# np.save(\"/content/drive/MyDrive/transformer_soc/scheduler_state.nph\",\n",
        "#         np.array([scheduler.learning_rate.numpy(), scheduler.T_cur, scheduler.T_i])\n",
        "#        )\n",
        "# print(f'''lr: {scheduler.learning_rate.numpy()}\n",
        "# T_cur: {scheduler.T_cur}\n",
        "# T_i: {scheduler.T_i}\n",
        "# ''')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5pSwH7QlT_I"
      },
      "source": [
        "<a id = \"val\"></a>\n",
        "# Validate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYr2y9eulT_I"
      },
      "source": [
        "**Dev Set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuY9saCblT_I"
      },
      "outputs": [],
      "source": [
        "visualize_dev = validate(model, test_dataloader, dev = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uLkWkLlT_I"
      },
      "source": [
        "**Entire Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjvsvbIllT_I"
      },
      "outputs": [],
      "source": [
        "x_set, y_set = rolling_split(file, G.window_size, train = False)\n",
        "\n",
        "x_set = tf.data.Dataset.from_tensor_slices(x_set)\n",
        "y_set = tf.data.Dataset.from_tensor_slices(y_set)\n",
        "\n",
        "set_dataloader = tf.data.Dataset.zip((x_set, y_set)).batch(G.batch_size, drop_remainder=True)\n",
        "\n",
        "visualize = validate(model, set_dataloader, dev = False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "colab": {
      "name": "transform_notebook.ipynb",
      "provenance": []
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}